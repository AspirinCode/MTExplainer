Changed directory to /home/dpk25/MolecularTransformer2/scripts/toy_model/sear_biased/convergence.

JobID: 27625906
======
Time: Wed 19 Aug 18:19:31 BST 2020
Running on master node: gpu-e-69
Current directory: /home/dpk25/MolecularTransformer2/scripts/toy_model/sear_biased/convergence

Nodes allocated:
================
gpu-e-69

numtasks=1, numnodes=1, mpi_tasks_per_node=1 (OMP_NUM_THREADS=1)

Executing command:
==================
./train.sh 

[2020-08-19 18:19:38,011 INFO]  * src vocab size = 20
[2020-08-19 18:19:38,011 INFO]  * tgt vocab size = 20
[2020-08-19 18:19:38,012 INFO] Building model...
[2020-08-19 18:19:44,245 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(20, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(20, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=256, out_features=20, bias=True)
    (1): Cast()
    (2): LogSoftmax()
  )
)
[2020-08-19 18:19:44,247 INFO] encoder: 5265920
[2020-08-19 18:19:44,247 INFO] decoder: 6320660
[2020-08-19 18:19:44,247 INFO] * number of parameters: 11586580
[2020-08-19 18:19:44,251 INFO] Starting training on GPU: [0]
[2020-08-19 18:19:44,251 INFO] Start training loop and validate every 10000 steps...
[2020-08-19 18:19:44,251 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 18:19:44,315 INFO] number of examples: 4960
[2020-08-19 18:19:55,858 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_78.pt
[2020-08-19 18:20:07,117 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_156.pt
[2020-08-19 18:20:18,519 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_234.pt
[2020-08-19 18:20:29,862 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_312.pt
[2020-08-19 18:20:41,222 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_390.pt
[2020-08-19 18:20:52,475 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_468.pt
[2020-08-19 18:20:54,618 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 18:20:54,659 INFO] number of examples: 4960
[2020-08-19 18:21:04,101 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_546.pt
[2020-08-19 18:21:15,529 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_624.pt
[2020-08-19 18:21:28,147 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_702.pt
[2020-08-19 18:21:39,436 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_780.pt
[2020-08-19 18:21:50,850 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_858.pt
[2020-08-19 18:22:02,140 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_936.pt
[2020-08-19 18:22:05,592 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 18:22:05,611 INFO] number of examples: 4960
[2020-08-19 18:22:11,613 INFO] Step 1000/ 2500; acc:  79.42; ppl:  1.61; xent: 0.48; lr: 0.00140; 2706/2183 tok/s;    147 sec
[2020-08-19 18:22:13,591 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_1014.pt
[2020-08-19 18:22:24,886 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_1092.pt
[2020-08-19 18:22:36,206 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_1170.pt
[2020-08-19 18:22:47,516 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_1248.pt
[2020-08-19 18:22:58,789 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_1326.pt
[2020-08-19 18:23:10,065 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_1404.pt
[2020-08-19 18:23:15,124 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 18:23:15,165 INFO] number of examples: 4960
[2020-08-19 18:23:21,526 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_1482.pt
[2020-08-19 18:23:32,986 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_1560.pt
[2020-08-19 18:23:44,304 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_1638.pt
[2020-08-19 18:23:55,611 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_1716.pt
[2020-08-19 18:24:06,858 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_1794.pt
[2020-08-19 18:24:18,180 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_1872.pt
[2020-08-19 18:24:24,839 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 18:24:24,923 INFO] number of examples: 4960
[2020-08-19 18:24:29,578 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_1950.pt
[2020-08-19 18:24:37,107 INFO] Step 2000/ 2500; acc:  85.11; ppl:  1.37; xent: 0.32; lr: 0.00279; 2740/2211 tok/s;    293 sec
[2020-08-19 18:24:41,045 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_2028.pt
[2020-08-19 18:24:53,405 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_2106.pt
[2020-08-19 18:25:04,775 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_2184.pt
[2020-08-19 18:25:15,938 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_2262.pt
[2020-08-19 18:25:27,716 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_2340.pt
[2020-08-19 18:25:35,996 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 18:25:36,034 INFO] number of examples: 4960
[2020-08-19 18:25:39,139 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_2418.pt
[2020-08-19 18:25:50,354 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_2496.pt
[2020-08-19 18:25:51,229 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv/toy_model_step_2500.pt
