Changed directory to /home/dpk25/MolecularTransformer2/scripts/toy_model/sear_supBiased/convergence.

JobID: 27626689
======
Time: Wed 19 Aug 19:11:30 BST 2020
Running on master node: gpu-e-2
Current directory: /home/dpk25/MolecularTransformer2/scripts/toy_model/sear_supBiased/convergence

Nodes allocated:
================
gpu-e-2

numtasks=1, numnodes=1, mpi_tasks_per_node=1 (OMP_NUM_THREADS=1)

Executing command:
==================
./train.sh 

[2020-08-19 19:11:37,691 INFO]  * src vocab size = 18
[2020-08-19 19:11:37,691 INFO]  * tgt vocab size = 18
[2020-08-19 19:11:37,691 INFO] Building model...
[2020-08-19 19:11:44,218 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(18, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(18, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=256, out_features=18, bias=True)
    (1): Cast()
    (2): LogSoftmax()
  )
)
[2020-08-19 19:11:44,220 INFO] encoder: 5265408
[2020-08-19 19:11:44,221 INFO] decoder: 6320146
[2020-08-19 19:11:44,221 INFO] * number of parameters: 11585554
[2020-08-19 19:11:44,230 INFO] Starting training on GPU: [0]
[2020-08-19 19:11:44,230 INFO] Start training loop and validate every 10000 steps...
[2020-08-19 19:11:44,230 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:11:44,297 INFO] number of examples: 4848
[2020-08-19 19:12:07,865 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_152.pt
[2020-08-19 19:12:31,295 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_304.pt
[2020-08-19 19:12:54,577 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_456.pt
[2020-08-19 19:12:56,098 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:12:56,144 INFO] number of examples: 4848
[2020-08-19 19:13:17,958 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_608.pt
[2020-08-19 19:13:41,379 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_760.pt
[2020-08-19 19:14:04,771 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_912.pt
[2020-08-19 19:14:07,527 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:14:07,546 INFO] number of examples: 4848
[2020-08-19 19:14:18,452 INFO] Step 1000/ 7600; acc:  80.29; ppl:  1.57; xent: 0.45; lr: 0.00140; 2590/2086 tok/s;    154 sec
[2020-08-19 19:14:28,132 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_1064.pt
[2020-08-19 19:14:51,342 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_1216.pt
[2020-08-19 19:15:15,433 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_1368.pt
[2020-08-19 19:15:19,349 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:15:19,392 INFO] number of examples: 4848
[2020-08-19 19:15:38,468 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_1520.pt
[2020-08-19 19:16:02,172 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_1672.pt
[2020-08-19 19:16:25,053 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_1824.pt
[2020-08-19 19:16:30,589 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:16:30,615 INFO] number of examples: 4848
[2020-08-19 19:16:48,255 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_1976.pt
[2020-08-19 19:16:52,083 INFO] Step 2000/ 7600; acc:  85.24; ppl:  1.37; xent: 0.31; lr: 0.00279; 2603/2096 tok/s;    308 sec
[2020-08-19 19:17:10,992 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_2128.pt
[2020-08-19 19:17:33,904 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_2280.pt
[2020-08-19 19:17:40,308 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:17:40,352 INFO] number of examples: 4848
[2020-08-19 19:17:56,858 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_2432.pt
[2020-08-19 19:18:20,800 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_2584.pt
[2020-08-19 19:18:43,556 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_2736.pt
[2020-08-19 19:18:51,155 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:18:51,176 INFO] number of examples: 4848
[2020-08-19 19:19:06,543 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_2888.pt
[2020-08-19 19:19:23,470 INFO] Step 3000/ 7600; acc:  84.53; ppl:  1.39; xent: 0.33; lr: 0.00228; 2645/2130 tok/s;    459 sec
[2020-08-19 19:19:29,403 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_3040.pt
[2020-08-19 19:19:52,480 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_3192.pt
[2020-08-19 19:20:01,433 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:20:01,479 INFO] number of examples: 4848
[2020-08-19 19:20:15,593 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_3344.pt
[2020-08-19 19:20:38,435 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_3496.pt
[2020-08-19 19:21:01,595 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_3648.pt
[2020-08-19 19:21:11,744 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:21:11,765 INFO] number of examples: 4848
[2020-08-19 19:21:24,605 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_3800.pt
[2020-08-19 19:21:47,513 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_3952.pt
[2020-08-19 19:21:54,983 INFO] Step 4000/ 7600; acc:  86.84; ppl:  1.31; xent: 0.27; lr: 0.00198; 2641/2127 tok/s;    611 sec
[2020-08-19 19:22:10,476 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_4104.pt
[2020-08-19 19:22:21,794 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:22:21,814 INFO] number of examples: 4848
[2020-08-19 19:22:33,476 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_4256.pt
[2020-08-19 19:22:56,446 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_4408.pt
[2020-08-19 19:23:19,362 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_4560.pt
[2020-08-19 19:23:31,913 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:23:31,933 INFO] number of examples: 4848
[2020-08-19 19:23:42,331 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_4712.pt
[2020-08-19 19:24:05,242 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_4864.pt
[2020-08-19 19:24:25,793 INFO] Step 5000/ 7600; acc:  86.33; ppl:  1.32; xent: 0.28; lr: 0.00177; 2655/2137 tok/s;    762 sec
[2020-08-19 19:24:28,156 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_5016.pt
[2020-08-19 19:24:41,859 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:24:41,880 INFO] number of examples: 4848
[2020-08-19 19:24:51,095 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_5168.pt
[2020-08-19 19:25:14,015 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_5320.pt
[2020-08-19 19:25:36,838 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_5472.pt
[2020-08-19 19:25:51,876 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:25:51,916 INFO] number of examples: 4848
[2020-08-19 19:25:59,763 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_5624.pt
[2020-08-19 19:26:22,732 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_5776.pt
[2020-08-19 19:26:45,495 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_5928.pt
[2020-08-19 19:26:56,471 INFO] Step 6000/ 7600; acc:  86.68; ppl:  1.30; xent: 0.26; lr: 0.00161; 2656/2138 tok/s;    912 sec
[2020-08-19 19:27:01,656 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:27:01,676 INFO] number of examples: 4848
[2020-08-19 19:27:08,323 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_6080.pt
[2020-08-19 19:27:31,085 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_6232.pt
[2020-08-19 19:27:53,755 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_6384.pt
[2020-08-19 19:28:11,013 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:28:11,079 INFO] number of examples: 4848
[2020-08-19 19:28:16,555 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_6536.pt
[2020-08-19 19:28:39,256 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_6688.pt
[2020-08-19 19:29:01,858 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_6840.pt
[2020-08-19 19:29:20,249 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:29:20,270 INFO] number of examples: 4848
[2020-08-19 19:29:24,552 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_6992.pt
[2020-08-19 19:29:26,177 INFO] Step 7000/ 7600; acc:  86.67; ppl:  1.31; xent: 0.27; lr: 0.00149; 2673/2152 tok/s;   1062 sec
[2020-08-19 19:29:47,377 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_7144.pt
[2020-08-19 19:30:09,933 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_7296.pt
[2020-08-19 19:30:29,732 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:30:29,776 INFO] number of examples: 4848
[2020-08-19 19:30:32,764 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_7448.pt
[2020-08-19 19:30:55,501 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv1/toy_model_step_7600.pt
[2020-08-19 19:30:57,084 INFO]  * src vocab size = 18
[2020-08-19 19:30:57,084 INFO]  * tgt vocab size = 18
[2020-08-19 19:30:57,084 INFO] Building model...
[2020-08-19 19:30:59,177 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(18, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(18, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=256, out_features=18, bias=True)
    (1): Cast()
    (2): LogSoftmax()
  )
)
[2020-08-19 19:30:59,179 INFO] encoder: 5265408
[2020-08-19 19:30:59,179 INFO] decoder: 6320146
[2020-08-19 19:30:59,179 INFO] * number of parameters: 11585554
[2020-08-19 19:30:59,183 INFO] Starting training on GPU: [0]
[2020-08-19 19:30:59,183 INFO] Start training loop and validate every 10000 steps...
[2020-08-19 19:30:59,183 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:30:59,207 INFO] number of examples: 4848
[2020-08-19 19:31:22,363 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_152.pt
[2020-08-19 19:31:45,667 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_304.pt
[2020-08-19 19:32:08,994 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_456.pt
[2020-08-19 19:32:10,527 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:32:10,572 INFO] number of examples: 4848
[2020-08-19 19:32:32,382 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_608.pt
[2020-08-19 19:32:55,617 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_760.pt
[2020-08-19 19:33:18,850 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_912.pt
[2020-08-19 19:33:21,559 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:33:21,578 INFO] number of examples: 4848
[2020-08-19 19:33:32,508 INFO] Step 1000/ 7600; acc:  80.29; ppl:  1.57; xent: 0.45; lr: 0.00140; 2606/2098 tok/s;    153 sec
[2020-08-19 19:33:42,163 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_1064.pt
[2020-08-19 19:34:05,497 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_1216.pt
[2020-08-19 19:34:28,765 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_1368.pt
[2020-08-19 19:34:32,705 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:34:32,750 INFO] number of examples: 4848
[2020-08-19 19:34:52,144 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_1520.pt
[2020-08-19 19:35:15,465 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_1672.pt
[2020-08-19 19:35:38,833 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_1824.pt
[2020-08-19 19:35:44,128 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:35:44,151 INFO] number of examples: 4848
[2020-08-19 19:36:02,161 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_1976.pt
[2020-08-19 19:36:06,122 INFO] Step 2000/ 7600; acc:  85.24; ppl:  1.37; xent: 0.31; lr: 0.00279; 2603/2096 tok/s;    307 sec
[2020-08-19 19:36:25,515 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_2128.pt
[2020-08-19 19:36:48,741 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_2280.pt
[2020-08-19 19:36:55,302 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:36:55,347 INFO] number of examples: 4848
[2020-08-19 19:37:12,153 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_2432.pt
[2020-08-19 19:37:35,396 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_2584.pt
[2020-08-19 19:37:58,634 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_2736.pt
[2020-08-19 19:38:06,322 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:38:06,342 INFO] number of examples: 4848
[2020-08-19 19:38:21,946 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_2888.pt
[2020-08-19 19:38:39,209 INFO] Step 3000/ 7600; acc:  84.53; ppl:  1.39; xent: 0.33; lr: 0.00228; 2616/2106 tok/s;    460 sec
[2020-08-19 19:38:45,227 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_3040.pt
[2020-08-19 19:39:08,466 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_3192.pt
[2020-08-19 19:39:17,403 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:39:17,451 INFO] number of examples: 4848
[2020-08-19 19:39:31,848 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_3344.pt
[2020-08-19 19:39:55,124 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_3496.pt
[2020-08-19 19:40:18,446 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_3648.pt
[2020-08-19 19:40:28,815 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:40:28,836 INFO] number of examples: 4848
[2020-08-19 19:40:41,868 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_3800.pt
[2020-08-19 19:41:05,220 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_3952.pt
[2020-08-19 19:41:12,851 INFO] Step 4000/ 7600; acc:  86.84; ppl:  1.31; xent: 0.27; lr: 0.00198; 2605/2098 tok/s;    614 sec
[2020-08-19 19:41:28,520 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_4104.pt
[2020-08-19 19:41:40,088 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:41:40,108 INFO] number of examples: 4848
[2020-08-19 19:41:51,964 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_4256.pt
[2020-08-19 19:42:15,256 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_4408.pt
[2020-08-19 19:42:38,587 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_4560.pt
[2020-08-19 19:42:51,302 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:42:51,323 INFO] number of examples: 4848
[2020-08-19 19:43:01,951 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_4712.pt
[2020-08-19 19:43:25,382 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_4864.pt
[2020-08-19 19:43:46,173 INFO] Step 5000/ 7600; acc:  86.33; ppl:  1.32; xent: 0.28; lr: 0.00177; 2611/2102 tok/s;    767 sec
[2020-08-19 19:43:48,611 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_5016.pt
[2020-08-19 19:44:02,589 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:44:02,610 INFO] number of examples: 4848
[2020-08-19 19:44:12,081 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_5168.pt
[2020-08-19 19:44:35,613 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_5320.pt
[2020-08-19 19:44:58,936 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_5472.pt
[2020-08-19 19:45:14,154 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:45:14,195 INFO] number of examples: 4848
[2020-08-19 19:45:22,216 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_5624.pt
[2020-08-19 19:45:45,423 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_5776.pt
[2020-08-19 19:46:08,401 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_5928.pt
[2020-08-19 19:46:19,408 INFO] Step 6000/ 7600; acc:  86.68; ppl:  1.30; xent: 0.26; lr: 0.00161; 2612/2103 tok/s;    920 sec
[2020-08-19 19:46:24,635 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:46:24,657 INFO] number of examples: 4848
[2020-08-19 19:46:31,415 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_6080.pt
[2020-08-19 19:46:54,422 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_6232.pt
[2020-08-19 19:47:17,348 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_6384.pt
[2020-08-19 19:47:34,909 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:47:34,952 INFO] number of examples: 4848
[2020-08-19 19:47:40,477 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_6536.pt
[2020-08-19 19:48:03,505 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_6688.pt
[2020-08-19 19:48:26,454 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_6840.pt
[2020-08-19 19:48:45,161 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:48:45,181 INFO] number of examples: 4848
[2020-08-19 19:48:49,521 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_6992.pt
[2020-08-19 19:48:51,051 INFO] Step 7000/ 7600; acc:  86.67; ppl:  1.31; xent: 0.27; lr: 0.00149; 2638/2125 tok/s;   1072 sec
[2020-08-19 19:49:12,563 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_7144.pt
[2020-08-19 19:49:35,539 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_7296.pt
[2020-08-19 19:49:55,547 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:49:55,590 INFO] number of examples: 4848
[2020-08-19 19:49:58,606 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_7448.pt
[2020-08-19 19:50:21,548 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv2/toy_model_step_7600.pt
[2020-08-19 19:50:23,317 INFO]  * src vocab size = 18
[2020-08-19 19:50:23,317 INFO]  * tgt vocab size = 18
[2020-08-19 19:50:23,317 INFO] Building model...
[2020-08-19 19:50:25,387 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(18, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(18, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=256, out_features=18, bias=True)
    (1): Cast()
    (2): LogSoftmax()
  )
)
[2020-08-19 19:50:25,389 INFO] encoder: 5265408
[2020-08-19 19:50:25,389 INFO] decoder: 6320146
[2020-08-19 19:50:25,389 INFO] * number of parameters: 11585554
[2020-08-19 19:50:25,393 INFO] Starting training on GPU: [0]
[2020-08-19 19:50:25,393 INFO] Start training loop and validate every 10000 steps...
[2020-08-19 19:50:25,393 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:50:25,416 INFO] number of examples: 4848
[2020-08-19 19:50:48,632 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_152.pt
[2020-08-19 19:51:11,971 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_304.pt
[2020-08-19 19:51:35,280 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_456.pt
[2020-08-19 19:51:36,813 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:51:36,858 INFO] number of examples: 4848
[2020-08-19 19:51:58,782 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_608.pt
[2020-08-19 19:52:22,206 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_760.pt
[2020-08-19 19:52:45,427 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_912.pt
[2020-08-19 19:52:48,233 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:52:48,253 INFO] number of examples: 4848
[2020-08-19 19:52:59,173 INFO] Step 1000/ 7600; acc:  80.29; ppl:  1.57; xent: 0.45; lr: 0.00140; 2598/2092 tok/s;    154 sec
[2020-08-19 19:53:08,732 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_1064.pt
[2020-08-19 19:53:31,602 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_1216.pt
[2020-08-19 19:53:55,042 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_1368.pt
[2020-08-19 19:53:58,913 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:53:58,956 INFO] number of examples: 4848
[2020-08-19 19:54:17,972 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_1520.pt
[2020-08-19 19:54:40,797 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_1672.pt
[2020-08-19 19:55:03,855 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_1824.pt
[2020-08-19 19:55:09,071 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:55:09,091 INFO] number of examples: 4848
[2020-08-19 19:55:26,839 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_1976.pt
[2020-08-19 19:55:30,734 INFO] Step 2000/ 7600; acc:  85.24; ppl:  1.37; xent: 0.31; lr: 0.00279; 2639/2125 tok/s;    305 sec
[2020-08-19 19:55:49,676 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_2128.pt
[2020-08-19 19:56:12,440 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_2280.pt
[2020-08-19 19:56:18,876 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:56:18,922 INFO] number of examples: 4848
[2020-08-19 19:56:35,427 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_2432.pt
[2020-08-19 19:56:58,326 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_2584.pt
[2020-08-19 19:57:21,281 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_2736.pt
[2020-08-19 19:57:28,876 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:57:28,897 INFO] number of examples: 4848
[2020-08-19 19:57:44,244 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_2888.pt
[2020-08-19 19:58:01,138 INFO] Step 3000/ 7600; acc:  84.53; ppl:  1.39; xent: 0.33; lr: 0.00228; 2663/2144 tok/s;    456 sec
[2020-08-19 19:58:07,099 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_3040.pt
[2020-08-19 19:58:30,108 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_3192.pt
[2020-08-19 19:58:38,909 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:58:38,957 INFO] number of examples: 4848
[2020-08-19 19:58:53,149 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_3344.pt
[2020-08-19 19:59:16,008 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_3496.pt
[2020-08-19 19:59:39,022 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_3648.pt
[2020-08-19 19:59:49,194 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 19:59:49,215 INFO] number of examples: 4848
[2020-08-19 20:00:02,072 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_3800.pt
[2020-08-19 20:00:25,063 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_3952.pt
[2020-08-19 20:00:32,518 INFO] Step 4000/ 7600; acc:  86.84; ppl:  1.31; xent: 0.27; lr: 0.00198; 2643/2129 tok/s;    607 sec
[2020-08-19 20:00:47,926 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_4104.pt
[2020-08-19 20:00:59,203 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:00:59,225 INFO] number of examples: 4848
[2020-08-19 20:01:10,936 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_4256.pt
[2020-08-19 20:01:33,991 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_4408.pt
[2020-08-19 20:01:56,986 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_4560.pt
[2020-08-19 20:02:09,564 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:02:09,632 INFO] number of examples: 4848
[2020-08-19 20:02:20,125 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_4712.pt
[2020-08-19 20:02:43,099 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_4864.pt
[2020-08-19 20:03:03,654 INFO] Step 5000/ 7600; acc:  86.33; ppl:  1.32; xent: 0.28; lr: 0.00177; 2649/2133 tok/s;    758 sec
[2020-08-19 20:03:06,049 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_5016.pt
[2020-08-19 20:03:19,794 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:03:19,814 INFO] number of examples: 4848
[2020-08-19 20:03:29,132 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_5168.pt
[2020-08-19 20:03:52,025 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_5320.pt
[2020-08-19 20:04:14,941 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_5472.pt
[2020-08-19 20:04:29,884 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:04:29,925 INFO] number of examples: 4848
[2020-08-19 20:04:37,783 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_5624.pt
[2020-08-19 20:05:00,510 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_5776.pt
[2020-08-19 20:05:23,278 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_5928.pt
[2020-08-19 20:05:34,189 INFO] Step 6000/ 7600; acc:  86.68; ppl:  1.30; xent: 0.26; lr: 0.00161; 2659/2140 tok/s;    909 sec
[2020-08-19 20:05:39,352 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:05:39,372 INFO] number of examples: 4848
[2020-08-19 20:05:46,087 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_6080.pt
[2020-08-19 20:06:08,801 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_6232.pt
[2020-08-19 20:06:31,487 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_6384.pt
[2020-08-19 20:06:48,754 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:06:48,795 INFO] number of examples: 4848
[2020-08-19 20:06:54,263 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_6536.pt
[2020-08-19 20:07:16,997 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_6688.pt
[2020-08-19 20:07:39,722 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_6840.pt
[2020-08-19 20:07:58,240 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:07:58,261 INFO] number of examples: 4848
[2020-08-19 20:08:02,580 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_6992.pt
[2020-08-19 20:08:04,007 INFO] Step 7000/ 7600; acc:  86.67; ppl:  1.31; xent: 0.27; lr: 0.00149; 2671/2151 tok/s;   1059 sec
[2020-08-19 20:08:25,165 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_7144.pt
[2020-08-19 20:08:47,808 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_7296.pt
[2020-08-19 20:09:07,613 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:09:07,657 INFO] number of examples: 4848
[2020-08-19 20:09:10,635 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_7448.pt
[2020-08-19 20:09:33,354 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv3/toy_model_step_7600.pt
[2020-08-19 20:09:34,765 INFO]  * src vocab size = 18
[2020-08-19 20:09:34,765 INFO]  * tgt vocab size = 18
[2020-08-19 20:09:34,765 INFO] Building model...
[2020-08-19 20:09:36,805 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(18, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(18, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=256, out_features=18, bias=True)
    (1): Cast()
    (2): LogSoftmax()
  )
)
[2020-08-19 20:09:36,807 INFO] encoder: 5265408
[2020-08-19 20:09:36,807 INFO] decoder: 6320146
[2020-08-19 20:09:36,807 INFO] * number of parameters: 11585554
[2020-08-19 20:09:36,812 INFO] Starting training on GPU: [0]
[2020-08-19 20:09:36,812 INFO] Start training loop and validate every 10000 steps...
[2020-08-19 20:09:36,812 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:09:36,835 INFO] number of examples: 4848
[2020-08-19 20:10:00,200 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_152.pt
[2020-08-19 20:10:23,577 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_304.pt
[2020-08-19 20:10:46,971 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_456.pt
[2020-08-19 20:10:48,562 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:10:48,608 INFO] number of examples: 4848
[2020-08-19 20:11:10,611 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_608.pt
[2020-08-19 20:11:34,068 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_760.pt
[2020-08-19 20:11:57,470 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_912.pt
[2020-08-19 20:12:00,250 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:12:00,270 INFO] number of examples: 4848
[2020-08-19 20:12:11,328 INFO] Step 1000/ 7600; acc:  80.29; ppl:  1.57; xent: 0.45; lr: 0.00140; 2585/2082 tok/s;    155 sec
[2020-08-19 20:12:21,077 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_1064.pt
[2020-08-19 20:12:44,541 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_1216.pt
[2020-08-19 20:13:07,875 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_1368.pt
[2020-08-19 20:13:11,853 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:13:11,898 INFO] number of examples: 4848
[2020-08-19 20:13:31,373 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_1520.pt
[2020-08-19 20:13:54,770 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_1672.pt
[2020-08-19 20:14:18,339 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_1824.pt
[2020-08-19 20:14:23,664 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:14:23,709 INFO] number of examples: 4848
[2020-08-19 20:14:41,892 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_1976.pt
[2020-08-19 20:14:45,880 INFO] Step 2000/ 7600; acc:  85.24; ppl:  1.37; xent: 0.31; lr: 0.00279; 2588/2083 tok/s;    309 sec
[2020-08-19 20:15:05,376 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_2128.pt
[2020-08-19 20:15:28,845 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_2280.pt
[2020-08-19 20:15:35,376 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:15:35,422 INFO] number of examples: 4848
[2020-08-19 20:15:52,366 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_2432.pt
[2020-08-19 20:16:15,903 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_2584.pt
[2020-08-19 20:16:39,300 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_2736.pt
[2020-08-19 20:16:47,043 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:16:47,064 INFO] number of examples: 4848
[2020-08-19 20:17:02,738 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_2888.pt
[2020-08-19 20:17:20,076 INFO] Step 3000/ 7600; acc:  84.53; ppl:  1.39; xent: 0.33; lr: 0.00228; 2597/2091 tok/s;    463 sec
[2020-08-19 20:17:26,161 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_3040.pt
[2020-08-19 20:17:49,507 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_3192.pt
[2020-08-19 20:17:58,562 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:17:58,610 INFO] number of examples: 4848
[2020-08-19 20:18:13,066 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_3344.pt
[2020-08-19 20:18:36,356 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_3496.pt
[2020-08-19 20:18:59,831 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_3648.pt
[2020-08-19 20:19:10,180 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:19:10,204 INFO] number of examples: 4848
[2020-08-19 20:19:23,371 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_3800.pt
[2020-08-19 20:19:46,772 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_3952.pt
[2020-08-19 20:19:54,433 INFO] Step 4000/ 7600; acc:  86.84; ppl:  1.31; xent: 0.27; lr: 0.00198; 2592/2088 tok/s;    618 sec
[2020-08-19 20:20:10,335 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_4104.pt
[2020-08-19 20:20:21,930 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:20:21,952 INFO] number of examples: 4848
[2020-08-19 20:20:33,897 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_4256.pt
[2020-08-19 20:20:57,346 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_4408.pt
[2020-08-19 20:21:20,932 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_4560.pt
[2020-08-19 20:21:33,775 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:21:33,795 INFO] number of examples: 4848
[2020-08-19 20:21:44,503 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_4712.pt
[2020-08-19 20:22:07,834 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_4864.pt
[2020-08-19 20:22:28,812 INFO] Step 5000/ 7600; acc:  86.33; ppl:  1.32; xent: 0.28; lr: 0.00177; 2593/2088 tok/s;    772 sec
[2020-08-19 20:22:31,243 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_5016.pt
[2020-08-19 20:22:45,313 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:22:45,334 INFO] number of examples: 4848
[2020-08-19 20:22:54,812 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_5168.pt
[2020-08-19 20:23:18,250 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_5320.pt
[2020-08-19 20:23:41,618 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_5472.pt
[2020-08-19 20:23:57,069 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:23:57,112 INFO] number of examples: 4848
[2020-08-19 20:24:05,201 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_5624.pt
[2020-08-19 20:24:28,764 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_5776.pt
[2020-08-19 20:24:52,022 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_5928.pt
[2020-08-19 20:25:03,187 INFO] Step 6000/ 7600; acc:  86.68; ppl:  1.30; xent: 0.26; lr: 0.00161; 2592/2087 tok/s;    926 sec
[2020-08-19 20:25:08,473 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:25:08,495 INFO] number of examples: 4848
[2020-08-19 20:25:15,324 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_6080.pt
[2020-08-19 20:25:38,541 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_6232.pt
[2020-08-19 20:26:01,869 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_6384.pt
[2020-08-19 20:26:19,614 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:26:19,657 INFO] number of examples: 4848
[2020-08-19 20:26:25,275 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_6536.pt
[2020-08-19 20:26:48,570 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_6688.pt
[2020-08-19 20:27:11,711 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_6840.pt
[2020-08-19 20:27:30,517 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:27:30,538 INFO] number of examples: 4848
[2020-08-19 20:27:34,942 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_6992.pt
[2020-08-19 20:27:36,536 INFO] Step 7000/ 7600; acc:  86.67; ppl:  1.31; xent: 0.27; lr: 0.00149; 2609/2101 tok/s;   1080 sec
[2020-08-19 20:27:58,181 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_7144.pt
[2020-08-19 20:28:21,601 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_7296.pt
[2020-08-19 20:28:41,661 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:28:41,705 INFO] number of examples: 4848
[2020-08-19 20:28:44,724 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_7448.pt
[2020-08-19 20:29:07,951 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv4/toy_model_step_7600.pt
[2020-08-19 20:29:09,574 INFO]  * src vocab size = 18
[2020-08-19 20:29:09,574 INFO]  * tgt vocab size = 18
[2020-08-19 20:29:09,574 INFO] Building model...
[2020-08-19 20:29:11,621 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(18, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(18, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=256, out_features=18, bias=True)
    (1): Cast()
    (2): LogSoftmax()
  )
)
[2020-08-19 20:29:11,623 INFO] encoder: 5265408
[2020-08-19 20:29:11,623 INFO] decoder: 6320146
[2020-08-19 20:29:11,623 INFO] * number of parameters: 11585554
[2020-08-19 20:29:11,628 INFO] Starting training on GPU: [0]
[2020-08-19 20:29:11,629 INFO] Start training loop and validate every 10000 steps...
[2020-08-19 20:29:11,629 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:29:11,652 INFO] number of examples: 4848
[2020-08-19 20:29:34,878 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_152.pt
[2020-08-19 20:29:58,179 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_304.pt
[2020-08-19 20:30:21,508 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_456.pt
[2020-08-19 20:30:23,039 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:30:23,083 INFO] number of examples: 4848
[2020-08-19 20:30:44,879 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_608.pt
[2020-08-19 20:31:08,152 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_760.pt
[2020-08-19 20:31:31,508 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_912.pt
[2020-08-19 20:31:34,302 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:31:34,321 INFO] number of examples: 4848
[2020-08-19 20:31:45,244 INFO] Step 1000/ 7600; acc:  80.29; ppl:  1.57; xent: 0.45; lr: 0.00140; 2601/2094 tok/s;    154 sec
[2020-08-19 20:31:54,832 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_1064.pt
[2020-08-19 20:32:17,945 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_1216.pt
[2020-08-19 20:32:40,998 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_1368.pt
[2020-08-19 20:32:44,934 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:32:44,978 INFO] number of examples: 4848
[2020-08-19 20:33:04,169 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_1520.pt
[2020-08-19 20:33:27,202 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_1672.pt
[2020-08-19 20:33:50,169 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_1824.pt
[2020-08-19 20:33:55,417 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:33:55,437 INFO] number of examples: 4848
[2020-08-19 20:34:13,198 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_1976.pt
[2020-08-19 20:34:17,131 INFO] Step 2000/ 7600; acc:  85.24; ppl:  1.37; xent: 0.31; lr: 0.00279; 2633/2120 tok/s;    306 sec
[2020-08-19 20:34:36,256 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_2128.pt
[2020-08-19 20:34:59,237 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_2280.pt
[2020-08-19 20:35:05,746 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:35:05,793 INFO] number of examples: 4848
[2020-08-19 20:35:22,404 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_2432.pt
[2020-08-19 20:35:45,312 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_2584.pt
[2020-08-19 20:36:08,275 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_2736.pt
[2020-08-19 20:36:15,855 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:36:15,877 INFO] number of examples: 4848
[2020-08-19 20:36:31,267 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_2888.pt
[2020-08-19 20:36:48,330 INFO] Step 3000/ 7600; acc:  84.53; ppl:  1.39; xent: 0.33; lr: 0.00228; 2649/2132 tok/s;    457 sec
[2020-08-19 20:36:54,304 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_3040.pt
[2020-08-19 20:37:17,263 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_3192.pt
[2020-08-19 20:37:26,082 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:37:26,129 INFO] number of examples: 4848
[2020-08-19 20:37:40,395 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_3344.pt
[2020-08-19 20:38:03,257 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_3496.pt
[2020-08-19 20:38:26,147 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_3648.pt
[2020-08-19 20:38:36,268 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:38:36,290 INFO] number of examples: 4848
[2020-08-19 20:38:49,164 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_3800.pt
[2020-08-19 20:39:12,272 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_3952.pt
[2020-08-19 20:39:19,699 INFO] Step 4000/ 7600; acc:  86.84; ppl:  1.31; xent: 0.27; lr: 0.00198; 2644/2130 tok/s;    608 sec
[2020-08-19 20:39:35,134 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_4104.pt
[2020-08-19 20:39:46,546 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:39:46,567 INFO] number of examples: 4848
[2020-08-19 20:39:58,254 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_4256.pt
[2020-08-19 20:40:21,145 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_4408.pt
[2020-08-19 20:40:44,053 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_4560.pt
[2020-08-19 20:40:56,570 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:40:56,591 INFO] number of examples: 4848
[2020-08-19 20:41:07,007 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_4712.pt
[2020-08-19 20:41:29,932 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_4864.pt
[2020-08-19 20:41:50,540 INFO] Step 5000/ 7600; acc:  86.33; ppl:  1.32; xent: 0.28; lr: 0.00177; 2654/2137 tok/s;    759 sec
[2020-08-19 20:41:52,936 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_5016.pt
[2020-08-19 20:42:06,602 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:42:06,623 INFO] number of examples: 4848
[2020-08-19 20:42:15,895 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_5168.pt
[2020-08-19 20:42:38,849 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_5320.pt
[2020-08-19 20:43:01,646 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_5472.pt
[2020-08-19 20:43:16,704 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:43:16,746 INFO] number of examples: 4848
[2020-08-19 20:43:24,667 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_5624.pt
[2020-08-19 20:43:47,590 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_5776.pt
[2020-08-19 20:44:10,326 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_5928.pt
[2020-08-19 20:44:21,289 INFO] Step 6000/ 7600; acc:  86.68; ppl:  1.30; xent: 0.26; lr: 0.00161; 2655/2137 tok/s;    910 sec
[2020-08-19 20:44:26,405 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:44:26,425 INFO] number of examples: 4848
[2020-08-19 20:44:33,072 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_6080.pt
[2020-08-19 20:44:55,811 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_6232.pt
[2020-08-19 20:45:18,617 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_6384.pt
[2020-08-19 20:45:35,852 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:45:35,897 INFO] number of examples: 4848
[2020-08-19 20:45:41,410 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_6536.pt
[2020-08-19 20:46:04,006 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_6688.pt
[2020-08-19 20:46:26,559 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_6840.pt
[2020-08-19 20:46:45,047 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:46:45,105 INFO] number of examples: 4848
[2020-08-19 20:46:49,377 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_6992.pt
[2020-08-19 20:46:50,846 INFO] Step 7000/ 7600; acc:  86.67; ppl:  1.31; xent: 0.27; lr: 0.00149; 2675/2154 tok/s;   1059 sec
[2020-08-19 20:47:12,047 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_7144.pt
[2020-08-19 20:47:34,915 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_7296.pt
[2020-08-19 20:47:54,706 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_supBiased/.train.0.pt
[2020-08-19 20:47:54,750 INFO] number of examples: 4848
[2020-08-19 20:47:57,748 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_7448.pt
[2020-08-19 20:48:20,540 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_supBiased_conv5/toy_model_step_7600.pt
