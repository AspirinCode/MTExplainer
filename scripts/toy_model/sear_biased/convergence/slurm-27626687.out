Changed directory to /home/dpk25/MolecularTransformer2/scripts/toy_model/sear_biased/convergence.

JobID: 27626687
======
Time: Wed 19 Aug 19:11:30 BST 2020
Running on master node: gpu-e-2
Current directory: /home/dpk25/MolecularTransformer2/scripts/toy_model/sear_biased/convergence

Nodes allocated:
================
gpu-e-2

numtasks=1, numnodes=1, mpi_tasks_per_node=1 (OMP_NUM_THREADS=1)

Executing command:
==================
./train.sh 

[2020-08-19 19:11:37,690 INFO]  * src vocab size = 20
[2020-08-19 19:11:37,690 INFO]  * tgt vocab size = 20
[2020-08-19 19:11:37,690 INFO] Building model...
[2020-08-19 19:11:44,407 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(20, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(20, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=256, out_features=20, bias=True)
    (1): Cast()
    (2): LogSoftmax()
  )
)
[2020-08-19 19:11:44,443 INFO] encoder: 5265920
[2020-08-19 19:11:44,443 INFO] decoder: 6320660
[2020-08-19 19:11:44,443 INFO] * number of parameters: 11586580
[2020-08-19 19:11:44,446 INFO] Starting training on GPU: [0]
[2020-08-19 19:11:44,446 INFO] Start training loop and validate every 10000 steps...
[2020-08-19 19:11:44,447 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:11:44,505 INFO] number of examples: 4960
[2020-08-19 19:12:08,704 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_156.pt
[2020-08-19 19:12:32,853 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_312.pt
[2020-08-19 19:12:56,925 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_468.pt
[2020-08-19 19:12:59,062 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:12:59,108 INFO] number of examples: 4960
[2020-08-19 19:13:21,186 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_624.pt
[2020-08-19 19:13:45,145 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_780.pt
[2020-08-19 19:14:09,172 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_936.pt
[2020-08-19 19:14:12,818 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:14:12,838 INFO] number of examples: 4960
[2020-08-19 19:14:19,233 INFO] Step 1000/ 7800; acc:  79.42; ppl:  1.61; xent: 0.48; lr: 0.00140; 2576/2078 tok/s;    155 sec
[2020-08-19 19:14:33,181 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_1092.pt
[2020-08-19 19:14:57,128 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_1248.pt
[2020-08-19 19:15:21,188 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_1404.pt
[2020-08-19 19:15:26,549 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:15:26,593 INFO] number of examples: 4960
[2020-08-19 19:15:45,323 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_1560.pt
[2020-08-19 19:16:10,467 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_1716.pt
[2020-08-19 19:16:34,416 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_1872.pt
[2020-08-19 19:16:41,551 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:16:41,571 INFO] number of examples: 4960
[2020-08-19 19:16:54,147 INFO] Step 2000/ 7800; acc:  85.11; ppl:  1.37; xent: 0.32; lr: 0.00279; 2573/2076 tok/s;    310 sec
[2020-08-19 19:16:58,373 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_2028.pt
[2020-08-19 19:17:22,248 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_2184.pt
[2020-08-19 19:17:46,214 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_2340.pt
[2020-08-19 19:17:54,998 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:17:55,045 INFO] number of examples: 4960
[2020-08-19 19:18:10,178 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_2496.pt
[2020-08-19 19:18:34,308 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_2652.pt
[2020-08-19 19:18:59,281 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_2808.pt
[2020-08-19 19:19:09,780 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:19:09,801 INFO] number of examples: 4960
[2020-08-19 19:19:23,365 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_2964.pt
[2020-08-19 19:19:29,137 INFO] Step 3000/ 7800; acc:  85.97; ppl:  1.35; xent: 0.30; lr: 0.00228; 2573/2076 tok/s;    465 sec
[2020-08-19 19:19:47,416 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_3120.pt
[2020-08-19 19:20:11,576 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_3276.pt
[2020-08-19 19:20:23,903 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:20:23,951 INFO] number of examples: 4960
[2020-08-19 19:20:35,820 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_3432.pt
[2020-08-19 19:21:00,962 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_3588.pt
[2020-08-19 19:21:24,894 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_3744.pt
[2020-08-19 19:21:38,875 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:21:38,896 INFO] number of examples: 4960
[2020-08-19 19:21:48,947 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_3900.pt
[2020-08-19 19:22:04,380 INFO] Step 4000/ 7800; acc:  86.28; ppl:  1.33; xent: 0.29; lr: 0.00198; 2570/2074 tok/s;    620 sec
[2020-08-19 19:22:12,845 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_4056.pt
[2020-08-19 19:22:36,724 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_4212.pt
[2020-08-19 19:22:52,326 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:22:52,374 INFO] number of examples: 4960
[2020-08-19 19:23:00,793 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_4368.pt
[2020-08-19 19:23:24,827 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_4524.pt
[2020-08-19 19:23:48,833 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_4680.pt
[2020-08-19 19:24:06,191 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:24:06,212 INFO] number of examples: 4960
[2020-08-19 19:24:12,915 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_4836.pt
[2020-08-19 19:24:36,892 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_4992.pt
[2020-08-19 19:24:38,462 INFO] Step 5000/ 7800; acc:  86.48; ppl:  1.33; xent: 0.28; lr: 0.00177; 2589/2089 tok/s;    774 sec
[2020-08-19 19:25:00,947 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_5148.pt
[2020-08-19 19:25:19,948 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:25:19,997 INFO] number of examples: 4960
[2020-08-19 19:25:25,038 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_5304.pt
[2020-08-19 19:25:49,034 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_5460.pt
[2020-08-19 19:26:13,088 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_5616.pt
[2020-08-19 19:26:33,971 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:26:33,992 INFO] number of examples: 4960
[2020-08-19 19:26:37,233 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_5772.pt
[2020-08-19 19:27:01,163 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_5928.pt
[2020-08-19 19:27:12,396 INFO] Step 6000/ 7800; acc:  85.83; ppl:  1.34; xent: 0.29; lr: 0.00161; 2592/2091 tok/s;    928 sec
[2020-08-19 19:27:25,163 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_6084.pt
[2020-08-19 19:27:47,613 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:27:47,635 INFO] number of examples: 4960
[2020-08-19 19:27:49,186 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_6240.pt
[2020-08-19 19:28:13,199 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_6396.pt
[2020-08-19 19:28:36,936 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_6552.pt
[2020-08-19 19:29:00,603 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_6708.pt
[2020-08-19 19:29:01,097 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:29:01,119 INFO] number of examples: 4960
[2020-08-19 19:29:24,307 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_6864.pt
[2020-08-19 19:29:44,979 INFO] Step 7000/ 7800; acc:  86.70; ppl:  1.31; xent: 0.27; lr: 0.00149; 2613/2108 tok/s;   1081 sec
[2020-08-19 19:29:47,970 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_7020.pt
[2020-08-19 19:30:11,591 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_7176.pt
[2020-08-19 19:30:13,760 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:30:13,781 INFO] number of examples: 4960
[2020-08-19 19:30:35,370 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_7332.pt
[2020-08-19 19:30:59,049 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_7488.pt
[2020-08-19 19:31:22,619 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_7644.pt
[2020-08-19 19:31:26,644 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:31:26,665 INFO] number of examples: 4960
[2020-08-19 19:31:46,451 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv1/toy_model_step_7800.pt
[2020-08-19 19:31:47,819 INFO]  * src vocab size = 20
[2020-08-19 19:31:47,819 INFO]  * tgt vocab size = 20
[2020-08-19 19:31:47,819 INFO] Building model...
[2020-08-19 19:31:49,895 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(20, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(20, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=256, out_features=20, bias=True)
    (1): Cast()
    (2): LogSoftmax()
  )
)
[2020-08-19 19:31:49,897 INFO] encoder: 5265920
[2020-08-19 19:31:49,897 INFO] decoder: 6320660
[2020-08-19 19:31:49,898 INFO] * number of parameters: 11586580
[2020-08-19 19:31:49,902 INFO] Starting training on GPU: [0]
[2020-08-19 19:31:49,902 INFO] Start training loop and validate every 10000 steps...
[2020-08-19 19:31:49,902 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:31:49,926 INFO] number of examples: 4960
[2020-08-19 19:32:13,621 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_156.pt
[2020-08-19 19:32:37,508 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_312.pt
[2020-08-19 19:33:01,328 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_468.pt
[2020-08-19 19:33:03,283 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:33:03,328 INFO] number of examples: 4960
[2020-08-19 19:33:25,273 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_624.pt
[2020-08-19 19:33:49,115 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_780.pt
[2020-08-19 19:34:12,986 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_936.pt
[2020-08-19 19:34:16,634 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:34:16,654 INFO] number of examples: 4960
[2020-08-19 19:34:23,011 INFO] Step 1000/ 7800; acc:  79.42; ppl:  1.61; xent: 0.48; lr: 0.00140; 2604/2101 tok/s;    153 sec
[2020-08-19 19:34:36,878 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_1092.pt
[2020-08-19 19:35:00,976 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_1248.pt
[2020-08-19 19:35:24,754 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_1404.pt
[2020-08-19 19:35:30,034 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:35:30,078 INFO] number of examples: 4960
[2020-08-19 19:35:48,708 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_1560.pt
[2020-08-19 19:36:12,568 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_1716.pt
[2020-08-19 19:36:36,294 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_1872.pt
[2020-08-19 19:36:43,434 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:36:43,454 INFO] number of examples: 4960
[2020-08-19 19:36:56,025 INFO] Step 2000/ 7800; acc:  85.11; ppl:  1.37; xent: 0.32; lr: 0.00279; 2605/2102 tok/s;    306 sec
[2020-08-19 19:37:00,252 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_2028.pt
[2020-08-19 19:37:24,142 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_2184.pt
[2020-08-19 19:37:47,994 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_2340.pt
[2020-08-19 19:37:56,915 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:37:56,960 INFO] number of examples: 4960
[2020-08-19 19:38:12,110 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_2496.pt
[2020-08-19 19:38:36,048 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_2652.pt
[2020-08-19 19:38:59,750 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_2808.pt
[2020-08-19 19:39:10,306 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:39:10,327 INFO] number of examples: 4960
[2020-08-19 19:39:23,752 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_2964.pt
[2020-08-19 19:39:29,473 INFO] Step 3000/ 7800; acc:  85.97; ppl:  1.35; xent: 0.30; lr: 0.00228; 2599/2097 tok/s;    460 sec
[2020-08-19 19:39:47,541 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_3120.pt
[2020-08-19 19:40:11,588 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_3276.pt
[2020-08-19 19:40:23,709 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:40:23,757 INFO] number of examples: 4960
[2020-08-19 19:40:35,570 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_3432.pt
[2020-08-19 19:40:59,443 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_3588.pt
[2020-08-19 19:41:23,312 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_3744.pt
[2020-08-19 19:41:37,308 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:41:37,330 INFO] number of examples: 4960
[2020-08-19 19:41:47,330 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_3900.pt
[2020-08-19 19:42:02,800 INFO] Step 4000/ 7800; acc:  86.28; ppl:  1.33; xent: 0.29; lr: 0.00198; 2602/2100 tok/s;    613 sec
[2020-08-19 19:42:11,226 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_4056.pt
[2020-08-19 19:42:35,059 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_4212.pt
[2020-08-19 19:42:50,567 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:42:50,615 INFO] number of examples: 4960
[2020-08-19 19:42:58,934 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_4368.pt
[2020-08-19 19:43:22,865 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_4524.pt
[2020-08-19 19:43:46,689 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_4680.pt
[2020-08-19 19:44:03,927 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:44:03,948 INFO] number of examples: 4960
[2020-08-19 19:44:10,625 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_4836.pt
[2020-08-19 19:44:34,619 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_4992.pt
[2020-08-19 19:44:36,178 INFO] Step 5000/ 7800; acc:  86.48; ppl:  1.33; xent: 0.28; lr: 0.00177; 2601/2099 tok/s;    766 sec
[2020-08-19 19:44:58,384 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_5148.pt
[2020-08-19 19:45:17,445 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:45:17,500 INFO] number of examples: 4960
[2020-08-19 19:45:22,498 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_5304.pt
[2020-08-19 19:45:46,439 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_5460.pt
[2020-08-19 19:46:10,298 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_5616.pt
[2020-08-19 19:46:30,980 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:46:31,001 INFO] number of examples: 4960
[2020-08-19 19:46:34,238 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_5772.pt
[2020-08-19 19:46:58,090 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_5928.pt
[2020-08-19 19:47:09,249 INFO] Step 6000/ 7800; acc:  85.83; ppl:  1.34; xent: 0.29; lr: 0.00161; 2606/2103 tok/s;    919 sec
[2020-08-19 19:47:21,932 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_6084.pt
[2020-08-19 19:47:44,270 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:47:44,292 INFO] number of examples: 4960
[2020-08-19 19:47:45,844 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_6240.pt
[2020-08-19 19:48:09,696 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_6396.pt
[2020-08-19 19:48:33,581 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_6552.pt
[2020-08-19 19:48:57,758 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_6708.pt
[2020-08-19 19:48:58,197 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:48:58,218 INFO] number of examples: 4960
[2020-08-19 19:49:21,623 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_6864.pt
[2020-08-19 19:49:42,455 INFO] Step 7000/ 7800; acc:  86.70; ppl:  1.31; xent: 0.27; lr: 0.00149; 2603/2100 tok/s;   1073 sec
[2020-08-19 19:49:45,472 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_7020.pt
[2020-08-19 19:50:09,306 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_7176.pt
[2020-08-19 19:50:11,413 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:50:11,434 INFO] number of examples: 4960
[2020-08-19 19:50:33,253 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_7332.pt
[2020-08-19 19:50:57,065 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_7488.pt
[2020-08-19 19:51:20,995 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_7644.pt
[2020-08-19 19:51:24,902 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:51:24,922 INFO] number of examples: 4960
[2020-08-19 19:51:44,712 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv2/toy_model_step_7800.pt
[2020-08-19 19:51:46,076 INFO]  * src vocab size = 20
[2020-08-19 19:51:46,076 INFO]  * tgt vocab size = 20
[2020-08-19 19:51:46,076 INFO] Building model...
[2020-08-19 19:51:48,127 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(20, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(20, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=256, out_features=20, bias=True)
    (1): Cast()
    (2): LogSoftmax()
  )
)
[2020-08-19 19:51:48,129 INFO] encoder: 5265920
[2020-08-19 19:51:48,129 INFO] decoder: 6320660
[2020-08-19 19:51:48,129 INFO] * number of parameters: 11586580
[2020-08-19 19:51:48,134 INFO] Starting training on GPU: [0]
[2020-08-19 19:51:48,134 INFO] Start training loop and validate every 10000 steps...
[2020-08-19 19:51:48,134 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:51:48,159 INFO] number of examples: 4960
[2020-08-19 19:52:11,117 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_156.pt
[2020-08-19 19:52:34,069 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_312.pt
[2020-08-19 19:52:56,991 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_468.pt
[2020-08-19 19:52:58,878 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:52:58,923 INFO] number of examples: 4960
[2020-08-19 19:53:20,045 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_624.pt
[2020-08-19 19:53:43,109 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_780.pt
[2020-08-19 19:54:06,106 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_936.pt
[2020-08-19 19:54:09,645 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:54:09,665 INFO] number of examples: 4960
[2020-08-19 19:54:15,768 INFO] Step 1000/ 7800; acc:  79.42; ppl:  1.61; xent: 0.48; lr: 0.00140; 2701/2179 tok/s;    148 sec
[2020-08-19 19:54:29,150 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_1092.pt
[2020-08-19 19:54:52,244 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_1248.pt
[2020-08-19 19:55:15,328 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_1404.pt
[2020-08-19 19:55:20,523 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:55:20,567 INFO] number of examples: 4960
[2020-08-19 19:55:38,408 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_1560.pt
[2020-08-19 19:56:01,494 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_1716.pt
[2020-08-19 19:56:24,534 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_1872.pt
[2020-08-19 19:56:31,439 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:56:31,460 INFO] number of examples: 4960
[2020-08-19 19:56:43,605 INFO] Step 2000/ 7800; acc:  85.11; ppl:  1.37; xent: 0.32; lr: 0.00279; 2696/2176 tok/s;    295 sec
[2020-08-19 19:56:47,659 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_2028.pt
[2020-08-19 19:57:10,604 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_2184.pt
[2020-08-19 19:57:33,662 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_2340.pt
[2020-08-19 19:57:42,122 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:57:42,791 INFO] number of examples: 4960
[2020-08-19 19:57:57,400 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_2496.pt
[2020-08-19 19:58:20,256 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_2652.pt
[2020-08-19 19:58:43,367 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_2808.pt
[2020-08-19 19:58:53,463 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 19:58:53,484 INFO] number of examples: 4960
[2020-08-19 19:59:06,411 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_2964.pt
[2020-08-19 19:59:11,984 INFO] Step 3000/ 7800; acc:  85.97; ppl:  1.35; xent: 0.30; lr: 0.00228; 2688/2168 tok/s;    444 sec
[2020-08-19 19:59:29,466 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_3120.pt
[2020-08-19 19:59:52,437 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_3276.pt
[2020-08-19 20:00:04,154 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:00:04,202 INFO] number of examples: 4960
[2020-08-19 20:00:15,606 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_3432.pt
[2020-08-19 20:00:38,700 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_3588.pt
[2020-08-19 20:01:01,679 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_3744.pt
[2020-08-19 20:01:15,170 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:01:15,192 INFO] number of examples: 4960
[2020-08-19 20:01:24,857 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_3900.pt
[2020-08-19 20:01:39,977 INFO] Step 4000/ 7800; acc:  86.28; ppl:  1.33; xent: 0.29; lr: 0.00198; 2696/2176 tok/s;    592 sec
[2020-08-19 20:01:48,152 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_4056.pt
[2020-08-19 20:02:11,330 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_4212.pt
[2020-08-19 20:02:26,500 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:02:26,548 INFO] number of examples: 4960
[2020-08-19 20:02:34,615 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_4368.pt
[2020-08-19 20:02:57,749 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_4524.pt
[2020-08-19 20:03:20,938 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_4680.pt
[2020-08-19 20:03:37,578 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:03:37,599 INFO] number of examples: 4960
[2020-08-19 20:03:44,073 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_4836.pt
[2020-08-19 20:04:07,277 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_4992.pt
[2020-08-19 20:04:08,785 INFO] Step 5000/ 7800; acc:  86.48; ppl:  1.33; xent: 0.28; lr: 0.00177; 2681/2163 tok/s;    741 sec
[2020-08-19 20:04:30,449 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_5148.pt
[2020-08-19 20:04:48,741 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:04:48,791 INFO] number of examples: 4960
[2020-08-19 20:04:53,596 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_5304.pt
[2020-08-19 20:05:16,557 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_5460.pt
[2020-08-19 20:05:39,429 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_5616.pt
[2020-08-19 20:05:59,209 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:05:59,231 INFO] number of examples: 4960
[2020-08-19 20:06:02,296 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_5772.pt
[2020-08-19 20:06:25,052 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_5928.pt
[2020-08-19 20:06:35,704 INFO] Step 6000/ 7800; acc:  85.83; ppl:  1.34; xent: 0.29; lr: 0.00161; 2716/2191 tok/s;    888 sec
[2020-08-19 20:06:47,731 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_6084.pt
[2020-08-19 20:07:09,113 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:07:09,134 INFO] number of examples: 4960
[2020-08-19 20:07:10,621 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_6240.pt
[2020-08-19 20:07:33,393 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_6396.pt
[2020-08-19 20:07:56,050 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_6552.pt
[2020-08-19 20:08:18,706 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_6708.pt
[2020-08-19 20:08:19,104 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:08:19,125 INFO] number of examples: 4960
[2020-08-19 20:08:41,350 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_6864.pt
[2020-08-19 20:09:01,182 INFO] Step 7000/ 7800; acc:  86.70; ppl:  1.31; xent: 0.27; lr: 0.00149; 2741/2211 tok/s;   1033 sec
[2020-08-19 20:09:04,058 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_7020.pt
[2020-08-19 20:09:26,712 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_7176.pt
[2020-08-19 20:09:28,801 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:09:28,822 INFO] number of examples: 4960
[2020-08-19 20:09:49,556 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_7332.pt
[2020-08-19 20:10:12,356 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_7488.pt
[2020-08-19 20:10:34,986 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_7644.pt
[2020-08-19 20:10:38,750 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:10:38,770 INFO] number of examples: 4960
[2020-08-19 20:10:57,718 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv3/toy_model_step_7800.pt
[2020-08-19 20:10:59,168 INFO]  * src vocab size = 20
[2020-08-19 20:10:59,168 INFO]  * tgt vocab size = 20
[2020-08-19 20:10:59,168 INFO] Building model...
[2020-08-19 20:11:01,244 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(20, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(20, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=256, out_features=20, bias=True)
    (1): Cast()
    (2): LogSoftmax()
  )
)
[2020-08-19 20:11:01,246 INFO] encoder: 5265920
[2020-08-19 20:11:01,246 INFO] decoder: 6320660
[2020-08-19 20:11:01,246 INFO] * number of parameters: 11586580
[2020-08-19 20:11:01,250 INFO] Starting training on GPU: [0]
[2020-08-19 20:11:01,250 INFO] Start training loop and validate every 10000 steps...
[2020-08-19 20:11:01,250 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:11:01,275 INFO] number of examples: 4960
[2020-08-19 20:11:25,135 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_156.pt
[2020-08-19 20:11:49,034 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_312.pt
[2020-08-19 20:12:12,948 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_468.pt
[2020-08-19 20:12:14,970 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:12:15,016 INFO] number of examples: 4960
[2020-08-19 20:12:36,980 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_624.pt
[2020-08-19 20:13:00,873 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_780.pt
[2020-08-19 20:13:24,825 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_936.pt
[2020-08-19 20:13:28,591 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:13:28,611 INFO] number of examples: 4960
[2020-08-19 20:13:35,020 INFO] Step 1000/ 7800; acc:  79.42; ppl:  1.61; xent: 0.48; lr: 0.00140; 2593/2092 tok/s;    154 sec
[2020-08-19 20:13:48,914 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_1092.pt
[2020-08-19 20:14:12,829 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_1248.pt
[2020-08-19 20:14:36,808 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_1404.pt
[2020-08-19 20:14:42,110 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:14:42,159 INFO] number of examples: 4960
[2020-08-19 20:15:00,775 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_1560.pt
[2020-08-19 20:15:24,742 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_1716.pt
[2020-08-19 20:15:48,749 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_1872.pt
[2020-08-19 20:15:55,896 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:15:55,917 INFO] number of examples: 4960
[2020-08-19 20:16:08,495 INFO] Step 2000/ 7800; acc:  85.11; ppl:  1.37; xent: 0.32; lr: 0.00279; 2597/2096 tok/s;    307 sec
[2020-08-19 20:16:12,741 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_2028.pt
[2020-08-19 20:16:36,617 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_2184.pt
[2020-08-19 20:17:00,507 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_2340.pt
[2020-08-19 20:17:09,397 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:17:09,443 INFO] number of examples: 4960
[2020-08-19 20:17:24,525 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_2496.pt
[2020-08-19 20:17:48,484 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_2652.pt
[2020-08-19 20:18:12,378 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_2808.pt
[2020-08-19 20:18:22,828 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:18:22,849 INFO] number of examples: 4960
[2020-08-19 20:18:36,266 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_2964.pt
[2020-08-19 20:18:42,170 INFO] Step 3000/ 7800; acc:  85.97; ppl:  1.35; xent: 0.30; lr: 0.00228; 2595/2094 tok/s;    461 sec
[2020-08-19 20:19:00,340 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_3120.pt
[2020-08-19 20:19:24,258 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_3276.pt
[2020-08-19 20:19:36,375 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:19:36,422 INFO] number of examples: 4960
[2020-08-19 20:19:48,234 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_3432.pt
[2020-08-19 20:20:12,420 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_3588.pt
[2020-08-19 20:20:36,249 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_3744.pt
[2020-08-19 20:20:50,159 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:20:50,181 INFO] number of examples: 4960
[2020-08-19 20:21:00,240 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_3900.pt
[2020-08-19 20:21:15,749 INFO] Step 4000/ 7800; acc:  86.28; ppl:  1.33; xent: 0.29; lr: 0.00198; 2598/2097 tok/s;    614 sec
[2020-08-19 20:21:24,222 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_4056.pt
[2020-08-19 20:21:48,253 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_4212.pt
[2020-08-19 20:22:03,802 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:22:03,850 INFO] number of examples: 4960
[2020-08-19 20:22:12,227 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_4368.pt
[2020-08-19 20:22:36,172 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_4524.pt
[2020-08-19 20:23:00,140 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_4680.pt
[2020-08-19 20:23:17,455 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:23:17,476 INFO] number of examples: 4960
[2020-08-19 20:23:24,163 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_4836.pt
[2020-08-19 20:23:48,027 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_4992.pt
[2020-08-19 20:23:49,534 INFO] Step 5000/ 7800; acc:  86.48; ppl:  1.33; xent: 0.28; lr: 0.00177; 2594/2093 tok/s;    768 sec
[2020-08-19 20:24:11,977 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_5148.pt
[2020-08-19 20:24:30,901 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:24:30,951 INFO] number of examples: 4960
[2020-08-19 20:24:36,045 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_5304.pt
[2020-08-19 20:25:00,037 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_5460.pt
[2020-08-19 20:25:24,050 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_5616.pt
[2020-08-19 20:25:44,749 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:25:44,770 INFO] number of examples: 4960
[2020-08-19 20:25:47,975 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_5772.pt
[2020-08-19 20:26:11,879 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_5928.pt
[2020-08-19 20:26:23,122 INFO] Step 6000/ 7800; acc:  85.83; ppl:  1.34; xent: 0.29; lr: 0.00161; 2598/2096 tok/s;    922 sec
[2020-08-19 20:26:35,840 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_6084.pt
[2020-08-19 20:26:58,224 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:26:58,246 INFO] number of examples: 4960
[2020-08-19 20:26:59,813 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_6240.pt
[2020-08-19 20:27:23,784 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_6396.pt
[2020-08-19 20:27:47,698 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_6552.pt
[2020-08-19 20:28:11,554 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_6708.pt
[2020-08-19 20:28:12,024 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:28:12,046 INFO] number of examples: 4960
[2020-08-19 20:28:35,518 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_6864.pt
[2020-08-19 20:28:56,477 INFO] Step 7000/ 7800; acc:  86.70; ppl:  1.31; xent: 0.27; lr: 0.00149; 2600/2097 tok/s;   1075 sec
[2020-08-19 20:28:59,498 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_7020.pt
[2020-08-19 20:29:23,285 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_7176.pt
[2020-08-19 20:29:25,362 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:29:25,383 INFO] number of examples: 4960
[2020-08-19 20:29:47,148 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_7332.pt
[2020-08-19 20:30:10,982 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_7488.pt
[2020-08-19 20:30:34,752 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_7644.pt
[2020-08-19 20:30:38,655 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:30:38,676 INFO] number of examples: 4960
[2020-08-19 20:30:58,441 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv4/toy_model_step_7800.pt
[2020-08-19 20:31:00,301 INFO]  * src vocab size = 20
[2020-08-19 20:31:00,301 INFO]  * tgt vocab size = 20
[2020-08-19 20:31:00,301 INFO] Building model...
[2020-08-19 20:31:02,361 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(20, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(20, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=256, out_features=20, bias=True)
    (1): Cast()
    (2): LogSoftmax()
  )
)
[2020-08-19 20:31:02,364 INFO] encoder: 5265920
[2020-08-19 20:31:02,364 INFO] decoder: 6320660
[2020-08-19 20:31:02,364 INFO] * number of parameters: 11586580
[2020-08-19 20:31:02,368 INFO] Starting training on GPU: [0]
[2020-08-19 20:31:02,368 INFO] Start training loop and validate every 10000 steps...
[2020-08-19 20:31:02,368 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:31:02,393 INFO] number of examples: 4960
[2020-08-19 20:31:26,369 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_156.pt
[2020-08-19 20:31:50,439 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_312.pt
[2020-08-19 20:32:14,597 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_468.pt
[2020-08-19 20:32:16,574 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:32:16,618 INFO] number of examples: 4960
[2020-08-19 20:32:38,733 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_624.pt
[2020-08-19 20:33:02,845 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_780.pt
[2020-08-19 20:33:26,809 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_936.pt
[2020-08-19 20:33:30,485 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:33:30,505 INFO] number of examples: 4960
[2020-08-19 20:33:36,934 INFO] Step 1000/ 7800; acc:  79.42; ppl:  1.61; xent: 0.48; lr: 0.00140; 2580/2081 tok/s;    155 sec
[2020-08-19 20:33:50,941 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_1092.pt
[2020-08-19 20:34:15,197 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_1248.pt
[2020-08-19 20:34:39,216 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_1404.pt
[2020-08-19 20:34:44,539 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:34:44,583 INFO] number of examples: 4960
[2020-08-19 20:35:03,302 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_1560.pt
[2020-08-19 20:35:27,455 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_1716.pt
[2020-08-19 20:35:51,416 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_1872.pt
[2020-08-19 20:35:58,605 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:35:58,626 INFO] number of examples: 4960
[2020-08-19 20:36:11,273 INFO] Step 2000/ 7800; acc:  85.11; ppl:  1.37; xent: 0.32; lr: 0.00279; 2583/2084 tok/s;    309 sec
[2020-08-19 20:36:15,545 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_2028.pt
[2020-08-19 20:36:39,612 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_2184.pt
[2020-08-19 20:37:03,615 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_2340.pt
[2020-08-19 20:37:12,419 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:37:12,465 INFO] number of examples: 4960
[2020-08-19 20:37:27,658 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_2496.pt
[2020-08-19 20:37:51,721 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_2652.pt
[2020-08-19 20:38:15,752 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_2808.pt
[2020-08-19 20:38:26,268 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:38:26,290 INFO] number of examples: 4960
[2020-08-19 20:38:39,819 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_2964.pt
[2020-08-19 20:38:45,549 INFO] Step 3000/ 7800; acc:  85.97; ppl:  1.35; xent: 0.30; lr: 0.00228; 2585/2086 tok/s;    463 sec
[2020-08-19 20:39:03,781 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_3120.pt
[2020-08-19 20:39:27,782 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_3276.pt
[2020-08-19 20:39:39,999 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:39:40,047 INFO] number of examples: 4960
[2020-08-19 20:39:51,974 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_3432.pt
[2020-08-19 20:40:16,061 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_3588.pt
[2020-08-19 20:40:40,215 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_3744.pt
[2020-08-19 20:40:54,196 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:40:54,231 INFO] number of examples: 4960
[2020-08-19 20:41:04,305 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_3900.pt
[2020-08-19 20:41:19,951 INFO] Step 4000/ 7800; acc:  86.28; ppl:  1.33; xent: 0.29; lr: 0.00198; 2584/2085 tok/s;    618 sec
[2020-08-19 20:41:28,481 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_4056.pt
[2020-08-19 20:41:52,510 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_4212.pt
[2020-08-19 20:42:08,229 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:42:08,278 INFO] number of examples: 4960
[2020-08-19 20:42:16,674 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_4368.pt
[2020-08-19 20:42:40,799 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_4524.pt
[2020-08-19 20:43:04,829 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_4680.pt
[2020-08-19 20:43:22,086 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:43:22,108 INFO] number of examples: 4960
[2020-08-19 20:43:28,812 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_4836.pt
[2020-08-19 20:43:52,891 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_4992.pt
[2020-08-19 20:43:54,473 INFO] Step 5000/ 7800; acc:  86.48; ppl:  1.33; xent: 0.28; lr: 0.00177; 2582/2083 tok/s;    772 sec
[2020-08-19 20:44:16,902 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_5148.pt
[2020-08-19 20:44:35,820 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:44:35,869 INFO] number of examples: 4960
[2020-08-19 20:44:40,894 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_5304.pt
[2020-08-19 20:45:04,745 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_5460.pt
[2020-08-19 20:45:28,408 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_5616.pt
[2020-08-19 20:45:49,584 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:45:49,642 INFO] number of examples: 4960
[2020-08-19 20:45:52,817 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_5772.pt
[2020-08-19 20:46:16,586 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_5928.pt
[2020-08-19 20:46:27,634 INFO] Step 6000/ 7800; acc:  85.83; ppl:  1.34; xent: 0.29; lr: 0.00161; 2605/2102 tok/s;    925 sec
[2020-08-19 20:46:40,154 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_6084.pt
[2020-08-19 20:47:02,315 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:47:02,335 INFO] number of examples: 4960
[2020-08-19 20:47:03,882 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_6240.pt
[2020-08-19 20:47:27,746 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_6396.pt
[2020-08-19 20:47:51,385 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_6552.pt
[2020-08-19 20:48:15,091 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_6708.pt
[2020-08-19 20:48:15,680 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:48:15,701 INFO] number of examples: 4960
[2020-08-19 20:48:38,272 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_6864.pt
[2020-08-19 20:48:57,874 INFO] Step 7000/ 7800; acc:  86.70; ppl:  1.31; xent: 0.27; lr: 0.00149; 2654/2141 tok/s;   1076 sec
[2020-08-19 20:49:00,719 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_7020.pt
[2020-08-19 20:49:23,310 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_7176.pt
[2020-08-19 20:49:25,344 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:49:25,364 INFO] number of examples: 4960
[2020-08-19 20:49:45,752 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_7332.pt
[2020-08-19 20:50:08,320 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_7488.pt
[2020-08-19 20:50:30,593 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_7644.pt
[2020-08-19 20:50:34,419 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_biased/.train.0.pt
[2020-08-19 20:50:34,439 INFO] number of examples: 4960
[2020-08-19 20:50:53,178 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_biased_conv5/toy_model_step_7800.pt
