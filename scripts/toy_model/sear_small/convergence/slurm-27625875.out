Changed directory to /home/dpk25/MolecularTransformer2/scripts/toy_model/sear_small/convergence.

JobID: 27625875
======
Time: Wed 19 Aug 17:59:34 BST 2020
Running on master node: gpu-e-50
Current directory: /home/dpk25/MolecularTransformer2/scripts/toy_model/sear_small/convergence

Nodes allocated:
================
gpu-e-50

numtasks=1, numnodes=1, mpi_tasks_per_node=1 (OMP_NUM_THREADS=1)

Executing command:
==================
./train.sh 

[2020-08-19 17:59:35,326 INFO]  * src vocab size = 20
[2020-08-19 17:59:35,326 INFO]  * tgt vocab size = 20
[2020-08-19 17:59:35,326 INFO] Building model...
[2020-08-19 17:59:41,944 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(20, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(20, 256, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (generator): Sequential(
    (0): Linear(in_features=256, out_features=20, bias=True)
    (1): Cast()
    (2): LogSoftmax()
  )
)
[2020-08-19 17:59:41,946 INFO] encoder: 5265920
[2020-08-19 17:59:41,946 INFO] decoder: 6320660
[2020-08-19 17:59:41,946 INFO] * number of parameters: 11586580
[2020-08-19 17:59:41,951 INFO] Starting training on GPU: [0]
[2020-08-19 17:59:41,951 INFO] Start training loop and validate every 10000 steps...
[2020-08-19 17:59:41,951 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_small/.train.0.pt
[2020-08-19 17:59:42,074 INFO] number of examples: 9920
[2020-08-19 17:59:58,400 INFO] Step 100/ 5000; acc:  51.84; ppl:  3.65; xent: 1.30; lr: 0.00014; 2394/1939 tok/s;     16 sec
[2020-08-19 18:00:07,111 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_156.pt
[2020-08-19 18:00:14,351 INFO] Step 200/ 5000; acc:  73.42; ppl:  1.83; xent: 0.61; lr: 0.00028; 2467/1997 tok/s;     32 sec
[2020-08-19 18:00:29,978 INFO] Step 300/ 5000; acc:  78.36; ppl:  1.63; xent: 0.49; lr: 0.00042; 2533/2056 tok/s;     48 sec
[2020-08-19 18:00:31,850 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_312.pt
[2020-08-19 18:00:45,855 INFO] Step 400/ 5000; acc:  82.00; ppl:  1.50; xent: 0.40; lr: 0.00056; 2474/2005 tok/s;     64 sec
[2020-08-19 18:00:56,475 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_468.pt
[2020-08-19 18:01:01,847 INFO] Step 500/ 5000; acc:  83.38; ppl:  1.45; xent: 0.37; lr: 0.00070; 2455/1998 tok/s;     80 sec
[2020-08-19 18:01:17,492 INFO] Step 600/ 5000; acc:  84.24; ppl:  1.42; xent: 0.35; lr: 0.00084; 2483/2013 tok/s;     96 sec
[2020-08-19 18:01:21,237 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_624.pt
[2020-08-19 18:01:33,468 INFO] Step 700/ 5000; acc:  85.60; ppl:  1.38; xent: 0.32; lr: 0.00098; 2483/2024 tok/s;    112 sec
[2020-08-19 18:01:45,985 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_780.pt
[2020-08-19 18:01:49,423 INFO] Step 800/ 5000; acc:  85.33; ppl:  1.38; xent: 0.32; lr: 0.00112; 2456/1994 tok/s;    127 sec
[2020-08-19 18:02:05,008 INFO] Step 900/ 5000; acc:  86.06; ppl:  1.36; xent: 0.31; lr: 0.00126; 2529/2055 tok/s;    143 sec
[2020-08-19 18:02:10,631 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_936.pt
[2020-08-19 18:02:21,008 INFO] Step 1000/ 5000; acc:  86.28; ppl:  1.34; xent: 0.30; lr: 0.00140; 2432/1976 tok/s;    159 sec
[2020-08-19 18:02:21,165 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_small/.train.0.pt
[2020-08-19 18:02:21,257 INFO] number of examples: 9920
[2020-08-19 18:02:35,523 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_1092.pt
[2020-08-19 18:02:37,197 INFO] Step 1100/ 5000; acc:  86.24; ppl:  1.35; xent: 0.30; lr: 0.00154; 2430/1967 tok/s;    175 sec
[2020-08-19 18:02:52,753 INFO] Step 1200/ 5000; acc:  86.65; ppl:  1.33; xent: 0.29; lr: 0.00168; 2533/2050 tok/s;    191 sec
[2020-08-19 18:03:00,166 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_1248.pt
[2020-08-19 18:03:08,553 INFO] Step 1300/ 5000; acc:  86.54; ppl:  1.34; xent: 0.29; lr: 0.00182; 2501/2031 tok/s;    207 sec
[2020-08-19 18:03:24,145 INFO] Step 1400/ 5000; acc:  86.63; ppl:  1.33; xent: 0.29; lr: 0.00196; 2522/2044 tok/s;    222 sec
[2020-08-19 18:03:24,773 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_1404.pt
[2020-08-19 18:03:40,088 INFO] Step 1500/ 5000; acc:  85.43; ppl:  1.38; xent: 0.32; lr: 0.00210; 2461/2004 tok/s;    238 sec
[2020-08-19 18:03:49,492 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_1560.pt
[2020-08-19 18:03:56,043 INFO] Step 1600/ 5000; acc:  85.34; ppl:  1.38; xent: 0.32; lr: 0.00224; 2435/1974 tok/s;    254 sec
[2020-08-19 18:04:11,638 INFO] Step 1700/ 5000; acc:  84.81; ppl:  1.39; xent: 0.33; lr: 0.00238; 2542/2072 tok/s;    270 sec
[2020-08-19 18:04:14,137 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_1716.pt
[2020-08-19 18:04:27,608 INFO] Step 1800/ 5000; acc:  84.31; ppl:  1.40; xent: 0.34; lr: 0.00252; 2455/1992 tok/s;    286 sec
[2020-08-19 18:04:38,835 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_1872.pt
[2020-08-19 18:04:43,495 INFO] Step 1900/ 5000; acc:  83.96; ppl:  1.42; xent: 0.35; lr: 0.00266; 2483/2018 tok/s;    302 sec
[2020-08-19 18:04:59,097 INFO] Step 2000/ 5000; acc:  84.19; ppl:  1.40; xent: 0.34; lr: 0.00279; 2495/2027 tok/s;    317 sec
[2020-08-19 18:04:59,409 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_small/.train.0.pt
[2020-08-19 18:04:59,496 INFO] number of examples: 9920
[2020-08-19 18:05:03,640 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_2028.pt
[2020-08-19 18:05:15,177 INFO] Step 2100/ 5000; acc:  84.19; ppl:  1.41; xent: 0.34; lr: 0.00273; 2445/1979 tok/s;    333 sec
[2020-08-19 18:05:28,268 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_2184.pt
[2020-08-19 18:05:31,040 INFO] Step 2200/ 5000; acc:  82.97; ppl:  1.43; xent: 0.36; lr: 0.00266; 2484/2010 tok/s;    349 sec
[2020-08-19 18:05:46,632 INFO] Step 2300/ 5000; acc:  84.97; ppl:  1.38; xent: 0.32; lr: 0.00261; 2533/2057 tok/s;    365 sec
[2020-08-19 18:05:52,872 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_2340.pt
[2020-08-19 18:06:02,562 INFO] Step 2400/ 5000; acc:  85.14; ppl:  1.36; xent: 0.31; lr: 0.00255; 2471/2002 tok/s;    381 sec
[2020-08-19 18:06:17,546 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_2496.pt
[2020-08-19 18:06:18,451 INFO] Step 2500/ 5000; acc:  84.87; ppl:  1.37; xent: 0.32; lr: 0.00250; 2469/2011 tok/s;    397 sec
[2020-08-19 18:06:34,044 INFO] Step 2600/ 5000; acc:  85.76; ppl:  1.35; xent: 0.30; lr: 0.00245; 2491/2017 tok/s;    412 sec
[2020-08-19 18:06:42,121 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_2652.pt
[2020-08-19 18:06:49,935 INFO] Step 2700/ 5000; acc:  85.50; ppl:  1.36; xent: 0.31; lr: 0.00241; 2494/2033 tok/s;    428 sec
[2020-08-19 18:07:05,480 INFO] Step 2800/ 5000; acc:  85.13; ppl:  1.37; xent: 0.32; lr: 0.00236; 2526/2051 tok/s;    444 sec
[2020-08-19 18:07:06,718 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_2808.pt
[2020-08-19 18:07:21,302 INFO] Step 2900/ 5000; acc:  85.74; ppl:  1.35; xent: 0.30; lr: 0.00232; 2491/2024 tok/s;    459 sec
[2020-08-19 18:07:31,254 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_2964.pt
[2020-08-19 18:07:37,448 INFO] Step 3000/ 5000; acc:  83.74; ppl:  1.41; xent: 0.35; lr: 0.00228; 2412/1960 tok/s;    475 sec
[2020-08-19 18:07:37,916 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_small/.train.0.pt
[2020-08-19 18:07:38,004 INFO] number of examples: 9920
[2020-08-19 18:07:53,117 INFO] Step 3100/ 5000; acc:  85.48; ppl:  1.35; xent: 0.30; lr: 0.00224; 2508/2030 tok/s;    491 sec
[2020-08-19 18:07:56,214 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_3120.pt
[2020-08-19 18:08:08,880 INFO] Step 3200/ 5000; acc:  85.79; ppl:  1.33; xent: 0.29; lr: 0.00221; 2501/2024 tok/s;    507 sec
[2020-08-19 18:08:20,595 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_3276.pt
[2020-08-19 18:08:24,683 INFO] Step 3300/ 5000; acc:  84.98; ppl:  1.37; xent: 0.32; lr: 0.00218; 2498/2029 tok/s;    523 sec
[2020-08-19 18:08:40,036 INFO] Step 3400/ 5000; acc:  85.80; ppl:  1.34; xent: 0.30; lr: 0.00214; 2565/2078 tok/s;    538 sec
[2020-08-19 18:08:44,965 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_3432.pt
[2020-08-19 18:08:55,807 INFO] Step 3500/ 5000; acc:  85.88; ppl:  1.33; xent: 0.29; lr: 0.00211; 2485/2024 tok/s;    554 sec
[2020-08-19 18:09:09,331 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_3588.pt
[2020-08-19 18:09:11,599 INFO] Step 3600/ 5000; acc:  86.33; ppl:  1.32; xent: 0.28; lr: 0.00208; 2461/1993 tok/s;    570 sec
[2020-08-19 18:09:26,972 INFO] Step 3700/ 5000; acc:  85.95; ppl:  1.34; xent: 0.29; lr: 0.00205; 2581/2104 tok/s;    585 sec
[2020-08-19 18:09:33,757 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_3744.pt
[2020-08-19 18:09:42,726 INFO] Step 3800/ 5000; acc:  86.06; ppl:  1.33; xent: 0.29; lr: 0.00203; 2491/2024 tok/s;    601 sec
[2020-08-19 18:09:58,092 INFO] Step 3900/ 5000; acc:  86.09; ppl:  1.33; xent: 0.29; lr: 0.00200; 2559/2078 tok/s;    616 sec
[2020-08-19 18:09:58,094 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_3900.pt
[2020-08-19 18:10:13,747 INFO] Step 4000/ 5000; acc:  85.51; ppl:  1.35; xent: 0.30; lr: 0.00198; 2493/2026 tok/s;    632 sec
[2020-08-19 18:10:14,362 INFO] Loading dataset from /home/dpk25/rds/hpc-work/toy_model/data_sear_small/.train.0.pt
[2020-08-19 18:10:14,453 INFO] number of examples: 9920
[2020-08-19 18:10:22,522 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_4056.pt
[2020-08-19 18:10:29,529 INFO] Step 4100/ 5000; acc:  84.02; ppl:  1.41; xent: 0.34; lr: 0.00195; 2488/2014 tok/s;    648 sec
[2020-08-19 18:10:44,852 INFO] Step 4200/ 5000; acc:  85.88; ppl:  1.33; xent: 0.28; lr: 0.00193; 2574/2081 tok/s;    663 sec
[2020-08-19 18:10:46,706 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_4212.pt
[2020-08-19 18:11:00,598 INFO] Step 4300/ 5000; acc:  86.16; ppl:  1.33; xent: 0.29; lr: 0.00191; 2507/2036 tok/s;    679 sec
[2020-08-19 18:11:11,040 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_4368.pt
[2020-08-19 18:11:16,318 INFO] Step 4400/ 5000; acc:  86.24; ppl:  1.32; xent: 0.28; lr: 0.00188; 2508/2032 tok/s;    694 sec
[2020-08-19 18:11:31,677 INFO] Step 4500/ 5000; acc:  85.81; ppl:  1.34; xent: 0.29; lr: 0.00186; 2550/2077 tok/s;    710 sec
[2020-08-19 18:11:35,380 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_4524.pt
[2020-08-19 18:11:47,424 INFO] Step 4600/ 5000; acc:  86.30; ppl:  1.33; xent: 0.28; lr: 0.00184; 2466/1997 tok/s;    725 sec
[2020-08-19 18:11:59,724 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_4680.pt
[2020-08-19 18:12:03,152 INFO] Step 4700/ 5000; acc:  86.51; ppl:  1.32; xent: 0.28; lr: 0.00182; 2520/2054 tok/s;    741 sec
[2020-08-19 18:12:18,514 INFO] Step 4800/ 5000; acc:  87.29; ppl:  1.29; xent: 0.26; lr: 0.00180; 2562/2082 tok/s;    757 sec
[2020-08-19 18:12:24,039 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_4836.pt
[2020-08-19 18:12:34,154 INFO] Step 4900/ 5000; acc:  87.63; ppl:  1.29; xent: 0.25; lr: 0.00179; 2510/2039 tok/s;    772 sec
[2020-08-19 18:12:48,306 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_4992.pt
[2020-08-19 18:12:49,964 INFO] Step 5000/ 5000; acc:  86.94; ppl:  1.30; xent: 0.26; lr: 0.00177; 2470/2009 tok/s;    788 sec
[2020-08-19 18:12:49,966 INFO] Saving checkpoint /home/dpk25/rds/hpc-work/toy_model/sear_models_small_conv/toy_model_step_5000.pt
